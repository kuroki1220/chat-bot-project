import os
import re
import time
import argparse
import datetime
import pandas as pd
import requests
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# ===== è¨­å®š =====
API_URL = "http://localhost:8000/chat"  # backend/main.py ã‚’èµ·å‹•ã—ã¦ãŠã
QA_CSV_PATH = os.path.join(os.path.dirname(__file__), "qa_data.csv")
RESULTS_DIR = os.path.join(os.path.dirname(__file__), "results")

# è»½é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆæ¯”è¼ƒç”¨ï¼‰
encoder = SentenceTransformer("paraphrase-MiniLM-L6-v2")

# â€œè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“â€ç³»ã®è¿”ç­”ã‚’æ¤œå‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
FALLBACK_PATTERNS = [
    "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“","è¦‹å½“ãŸã‚Šã¾ã›ã‚“ã§ã—ãŸ",
    "ã‚ã‹ã‚Šã¾ã›ã‚“", "åˆ†ã‹ã‚Šã¾ã›ã‚“", "ã‚ã‹ã‚Šã‹ã­ã¾ã™", "ã‚ã‹ã‚Šå…¼ã­ã¾ã™",
    "ä»–ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãŠè©¦ã—ãã ã•ã„", "internal server error",
    "sorry", "i don't know", "no relevant information"
]

def is_not_found(text: str) -> bool:
    """â€œè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“â€ç³»ã®è¿”ç­”ã‚’æ¤œå‡ºï¼ˆç©ºæ–‡å­—ã‚‚ä¸æ­£è§£æ‰±ã„ï¼‰"""
    if not text:
        return True
    t = re.sub(r"\s+", "", text.lower())  # ç©ºç™½ã‚’é™¤å»ã—ã¦å°æ–‡å­—åŒ–
    for pat in FALLBACK_PATTERNS:
        if pat.lower().replace(" ", "") in t:
            return True
    return False

def semantic_score(text1: str, text2: str) -> float:
    """æ–‡ç« åŒå£«ã®æ„å‘³é¡ä¼¼åº¦ï¼ˆ0.0ã€œ1.0ï¼‰"""
    if not text1 or not text2:
        return 0.0
    emb1 = encoder.encode([text1], convert_to_tensor=True)
    emb2 = encoder.encode([text2], convert_to_tensor=True)
    sim = cosine_similarity(emb1.cpu().numpy(), emb2.cpu().numpy())[0][0]
    return round(float(sim), 3)

def main(limit: int, threshold: float, sleep_sec: float):
    # ===== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
    qa_df = pd.read_csv(QA_CSV_PATH)
    if "è³ªå•" not in qa_df.columns or "å›ç­”" not in qa_df.columns:
        raise ValueError("CSVã«ã€Œè³ªå•ã€ã€Œå›ç­”ã€åˆ—ãŒå¿…è¦ã§ã™")
    qa_df = qa_df.dropna(subset=["è³ªå•", "å›ç­”"])
    if limit:
        qa_df = qa_df.head(limit)

    print(f"ğŸ“„ Q&Aãƒ‡ãƒ¼ã‚¿ {len(qa_df)} ä»¶ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼ˆé–¾å€¤ {threshold}ï¼‰")

    # ===== å‡ºåŠ›å…ˆæº–å‚™ï¼ˆå±¥æ­´ + æœ€æ–°ï¼‰=====
    os.makedirs(RESULTS_DIR, exist_ok=True)
    date_dir = os.path.join(RESULTS_DIR, datetime.datetime.now().strftime("%Y%m%d"))
    os.makedirs(date_dir, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path_ts = os.path.join(date_dir, f"test_results_{timestamp}.csv")
    out_path_latest = os.path.join(RESULTS_DIR, "test_results_latest.csv")

    results = []
    correct = 0

    start_time = time.time()  # ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²

    for idx, row in qa_df.iterrows():
        q = str(row["è³ªå•"]).strip()
        expected = str(row["å›ç­”"]).strip()
        print(f"\nğŸŸ¢ è³ªå• {idx}: {q}")

        # ===== APIå‘¼ã³å‡ºã— =====
        try:
            r = requests.post(API_URL, json={"message": q, "user_id": "eval"})
            r.raise_for_status()
            actual = r.json().get("response", "").strip()
        except Exception as e:
            print(f"âŒ APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
            actual = f"[APIã‚¨ãƒ©ãƒ¼] {e}"

        # ===== ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆâ€œè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“â€ç³»ã¯å¼·åˆ¶ 0.0ï¼‰=====
        if is_not_found(actual):
            score = 0.0
        else:
            score = semantic_score(expected, actual)

        judge = "ã€‡" if score >= threshold else "Ã—"
        if judge == "ã€‡":
            correct += 1

        print(f"æœŸå¾…å›ç­”: {expected[:60]}...")
        print(f"å®Ÿéš›å›ç­”: {actual[:60]}...")
        print(f"ã‚¹ã‚³ã‚¢: {score} åˆ¤å®š: {judge}")

        results.append({
            "No": idx,
            "è³ªå•": q,
            "æœŸå¾…å›ç­”": expected,
            "å®Ÿéš›å›ç­”": actual,
            "ã‚¹ã‚³ã‚¢": score,
            "åˆ¤å®š": judge
        })

        time.sleep(sleep_sec)  # APIåˆ¶é™å¯¾ç­–

    # ===== çµæœä¿å­˜ï¼ˆå±¥æ­´ + æœ€æ–°ï¼‰=====
    df = pd.DataFrame(results)
    df.to_csv(out_path_ts, index=False, encoding="utf-8-sig")
    df.to_csv(out_path_latest, index=False, encoding="utf-8-sig")

    # æ­£ç­”ç‡è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤ºï¼‰
    acc = round(correct / len(df) * 100, 1) if len(df) else 0.0
    print(f"\nâœ… å®Œäº†: {len(df)}ä»¶ / æ­£è§£ {correct}ä»¶ / æ­£ç­”ç‡ {acc}%")
    
    # çµŒéæ™‚é–“è¡¨ç¤º
    end_time = time.time()  # ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
    elapsed_time = end_time - start_time  # çµŒéæ™‚é–“ï¼ˆç§’ï¼‰
    
    # çµŒéæ™‚é–“ã‚’åˆ†ãƒ»ç§’ãƒ»æ™‚é–“ã®çµ„ã¿åˆã‚ã›ã§è¡¨ç¤º
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    hours = int(minutes // 60)
    minutes = minutes % 60

    elapsed_time_str = ""
    if hours > 0:
        elapsed_time_str += f"{hours}h"
    if minutes > 0:
        elapsed_time_str += f"{minutes}m"
    if seconds > 0:
        elapsed_time_str += f"{seconds}s"

    print(f"â³ æ™‚é–“: {elapsed_time_str}")  # çµŒéæ™‚é–“ã‚’è¡¨ç¤º

    print(f"ğŸ—‚ å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«: {out_path_ts}")
    print(f"ğŸ“Œ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«: {out_path_latest}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, default=10, help="ãƒ†ã‚¹ãƒˆä»¶æ•°ï¼ˆå…ˆé ­ã‹ã‚‰ï¼‰")
    parser.add_argument("--threshold", type=float, default=0.6, help="æ­£è§£åˆ¤å®šã®ã‚¹ã‚³ã‚¢é–¾å€¤")
    parser.add_argument("--sleep", type=float, default=0.5, help="APIå‘¼ã³å‡ºã—ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ç§’")
    args = parser.parse_args()
    main(args.limit, args.threshold, args.sleep)
